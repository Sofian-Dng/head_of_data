{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae03c1d0-304d-40a8-b15e-bd86896faf4a",
      "metadata": {
        "id": "ae03c1d0-304d-40a8-b15e-bd86896faf4a"
      },
      "source": [
        "# print('hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b4d3e7-00e6-4c7a-9109-4e236637bcd0",
      "metadata": {
        "id": "a5b4d3e7-00e6-4c7a-9109-4e236637bcd0"
      },
      "outputs": [],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13eb125-4f7a-4168-885a-0270d92a829f",
      "metadata": {
        "id": "d13eb125-4f7a-4168-885a-0270d92a829f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "\n",
        "# worldcloud\n",
        "import random\n",
        "from wordcloud import WordCloud\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a813b9-51e6-41ee-8e3d-90b8b2bda6bd",
      "metadata": {
        "id": "22a813b9-51e6-41ee-8e3d-90b8b2bda6bd"
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources (only needed once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcc74807-851f-43db-971d-2f356c6fc24b",
      "metadata": {
        "id": "dcc74807-851f-43db-971d-2f356c6fc24b"
      },
      "outputs": [],
      "source": [
        "# retrieving the datasets we used for the last notebook\n",
        "\n",
        "raw_path = '../data/olist_datasets/'\n",
        "\n",
        "df_customer = pd.read_csv(raw_path + 'olist_customers_dataset.csv')\n",
        "df_geolocation = pd.read_csv(raw_path + 'olist_geolocation_dataset.csv')\n",
        "df_orders = pd.read_csv(raw_path + 'olist_orders_dataset.csv')\n",
        "df_order_items = pd.read_csv(raw_path + 'olist_order_items_dataset.csv')\n",
        "df_order_payments = pd.read_csv(raw_path + 'olist_order_payments_dataset.csv')\n",
        "df_order_reviews = pd.read_csv(raw_path + 'olist_order_reviews_dataset.csv')\n",
        "df_products = pd.read_csv(raw_path + 'olist_products_dataset.csv')\n",
        "df_sellers = pd.read_csv(raw_path + 'olist_sellers_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d871906b-4a63-4a9a-b250-91354a37dd93",
      "metadata": {
        "id": "d871906b-4a63-4a9a-b250-91354a37dd93"
      },
      "source": [
        "# I - Sentiment analysis of customer reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f86f5ceb-b634-42a1-be04-c6c6751d1295",
      "metadata": {
        "id": "f86f5ceb-b634-42a1-be04-c6c6751d1295"
      },
      "source": [
        "## A - Reminders on the Review dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0482e24-8d04-49dd-a3a1-0c0ce8e574ad",
      "metadata": {
        "id": "d0482e24-8d04-49dd-a3a1-0c0ce8e574ad"
      },
      "outputs": [],
      "source": [
        "# checking that df_order_reviews is loaded\n",
        "\n",
        "df_order_reviews.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca108f21-1c12-4df4-b8b9-21bb6137ef75",
      "metadata": {
        "id": "ca108f21-1c12-4df4-b8b9-21bb6137ef75"
      },
      "outputs": [],
      "source": [
        "# Create a distribution plot (distplot) of review scores using Seaborn\n",
        "\n",
        "# Define professional color palette\n",
        "palette_colors = ['#2C3E50', '#E74C3C', '#ECF0F1', '#3498DB', '#2ECC71', '#F1C40F', '#9B59B6']\n",
        "\n",
        "# Set up the figure\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot the histogram\n",
        "sns.histplot(\n",
        "    data=df_order_reviews,\n",
        "    x='review_score',\n",
        "    bins=5,\n",
        "    edgecolor='black',\n",
        "    stat='count',\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_title('Distribution of Review Scores', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Review Score', fontsize=14)\n",
        "ax.set_ylabel('Frequency', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e917a1d0-1c40-42df-8e6f-a0fc397fa70b",
      "metadata": {
        "id": "e917a1d0-1c40-42df-8e6f-a0fc397fa70b"
      },
      "outputs": [],
      "source": [
        "# Checking for missing values in review comments\n",
        "missing_comments = df_order_reviews[['review_comment_title', 'review_comment_message']].isnull().sum()\n",
        "print(missing_comments)\n",
        "\n",
        "# Dropping rows where both comment fields are missing\n",
        "df_reviews_clean = df_order_reviews.dropna(subset=['review_comment_title', 'review_comment_message'], how='all')\n",
        "\n",
        "# Define a function to categorize review sentiment\n",
        "def classify_sentiment(score):\n",
        "    \"\"\"Classify sentiment based on review score.\"\"\"\n",
        "    if score >= 4:\n",
        "        return 'positive'\n",
        "    elif score <= 2:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply the sentiment classification\n",
        "df_order_reviews['sentiment'] = df_order_reviews['review_score'].apply(classify_sentiment)\n",
        "\n",
        "# Display the updated dataframe\n",
        "df_order_reviews\n",
        "\n",
        "# Filter again to remove any missing comments before analysis\n",
        "df_order_reviews = df_order_reviews.dropna(subset=['review_comment_message', 'review_comment_title'])\n",
        "df_order_reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb7a9ee-3c1a-469b-89b5-d3b5e7bd38f2",
      "metadata": {
        "id": "7fb7a9ee-3c1a-469b-89b5-d3b5e7bd38f2"
      },
      "outputs": [],
      "source": [
        "## Wordcloud\n",
        "\n",
        "# Generate a WordCloud for orders with negative reviews\n",
        "\n",
        "# Define a custom color function\n",
        "def random_color(word, font_size, position, orientation=None, random_state=42, **kwargs):\n",
        "    return random.choice(palette_colors)\n",
        "\n",
        "# Create the wordcloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Focus only on negative reviews\n",
        "wordcloud = WordCloud(\n",
        "    width=800, \n",
        "    height=400, \n",
        "    background_color='white', \n",
        "    color_func=random_color\n",
        ")\n",
        "\n",
        "# Generate wordcloud from review comments\n",
        "wordcloud.generate(' '.join(df_order_reviews['review_comment_message'].dropna()))\n",
        "\n",
        "# Display the wordcloud\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Review Comments', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85b7f32f-5d9d-441e-85bc-f42800f0eedb",
      "metadata": {
        "id": "85b7f32f-5d9d-441e-85bc-f42800f0eedb"
      },
      "source": [
        "## B - Text Preprocessing and conversion to numerical features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7a3330-bb10-4f9a-b4ac-90d46a409eca",
      "metadata": {
        "id": "1e7a3330-bb10-4f9a-b4ac-90d46a409eca"
      },
      "source": [
        "### 1 - Cleaning a small sample of the data and looking at the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222d9c7b-d4ab-48b3-9f06-6f156dadf6fd",
      "metadata": {
        "id": "222d9c7b-d4ab-48b3-9f06-6f156dadf6fd"
      },
      "outputs": [],
      "source": [
        "# Create a sample dataframe from df_order_reviews\n",
        "# Filtering rows with non-null review_comment_message\n",
        "# Keeping only review_id, review_score, and review_comment_message\n",
        "\n",
        "df_reviews_sample = df_order_reviews[\n",
        "    df_order_reviews['review_comment_message'].notnull()\n",
        "][['review_id', 'review_score', 'review_comment_message']].sample(frac=0.1)\n",
        "\n",
        "df_reviews_sample\n",
        "\n",
        "# Create a new column to lower-case the text\n",
        "\n",
        "df_reviews_sample['comment_clean'] = df_reviews_sample['review_comment_message'].apply(lambda x: x.lower())\n",
        "\n",
        "# Remove special characters using regex\n",
        "\n",
        "df_reviews_sample['comment_clean'] = df_reviews_sample['comment_clean'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "df_reviews_sample.head(20)\n",
        "\n",
        "# Tokenize the text into lists of words using NLTK\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "df_reviews_sample['comment_clean_tokenized'] = df_reviews_sample['comment_clean'].apply(word_tokenize)\n",
        "\n",
        "df_reviews_sample\n",
        "\n",
        "# Remove Portuguese stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "portuguese_stopwords = set(stopwords.words('portuguese'))\n",
        "df_reviews_sample['comment_clean_tokenized'] = df_reviews_sample['comment_clean_tokenized'].apply(\n",
        "    lambda tokens: [word for word in tokens if word not in portuguese_stopwords]\n",
        ")\n",
        "\n",
        "df_reviews_sample\n",
        "\n",
        "# Apply stemming to reduce words to their root form\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "df_reviews_sample['comment_clean_stemmed'] = df_reviews_sample['comment_clean_tokenized'].apply(\n",
        "    lambda tokens: [stemmer.stem(word) for word in tokens]\n",
        ")\n",
        "\n",
        "df_reviews_sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf0bee1-ae27-4817-958b-6c3a34698bcb",
      "metadata": {
        "id": "abf0bee1-ae27-4817-958b-6c3a34698bcb"
      },
      "source": [
        "### 2 - Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "047dbf55-c2cb-4795-8117-eb7b26d21def",
      "metadata": {
        "id": "047dbf55-c2cb-4795-8117-eb7b26d21def"
      },
      "outputs": [],
      "source": [
        "# Define a function to clean and preprocess text data\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove punctuation, numbers, and special characters\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Tokenize the text\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        # Remove Portuguese stopwords\n",
        "        portuguese_stopwords = set(stopwords.words('portuguese'))\n",
        "        words = [word for word in words if word not in portuguese_stopwords]\n",
        "\n",
        "        # Apply stemming\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "        # Rebuild the cleaned text\n",
        "        cleaned_text = ' '.join(stemmed_words)\n",
        "\n",
        "        return cleaned_text\n",
        "    else:\n",
        "        return ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34456f21-cee8-4575-b163-4c63614cd838",
      "metadata": {
        "id": "34456f21-cee8-4575-b163-4c63614cd838"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to the review comments\n",
        "\n",
        "df_reviews_sample['comment_clean'] = df_reviews_sample['review_comment_message'].apply(lambda x: clean_text(x))\n",
        "\n",
        "# Display a few examples of original and processed comments\n",
        "\n",
        "df_reviews_sample.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13e5e9f7-2b84-41aa-9f43-fa795efa9157",
      "metadata": {
        "id": "13e5e9f7-2b84-41aa-9f43-fa795efa9157"
      },
      "source": [
        "### 3 - Final preparation steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06168c58-5bf4-46e7-850c-be27addca044",
      "metadata": {
        "id": "06168c58-5bf4-46e7-850c-be27addca044"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing subsets\n",
        "\n",
        "train_data, test_data = train_test_split(df_reviews_sample, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the size of each subset\n",
        "print(f\"Train set dimensions: {train_data.shape}\")\n",
        "print(f\"Test set dimensions: {test_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59ef6766-4d6b-4404-8ebd-210c3aae3411",
      "metadata": {
        "id": "59ef6766-4d6b-4404-8ebd-210c3aae3411"
      },
      "source": [
        "## C - Building a Simple Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8bd7c7b-39c9-477f-9bc0-8141812aa504",
      "metadata": {
        "id": "b8bd7c7b-39c9-477f-9bc0-8141812aa504"
      },
      "outputs": [],
      "source": [
        "# Use CountVectorizer to transform text into a matrix of token counts\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5000)  # Keep only the top 5000 most frequent words\n",
        "\n",
        "# Fit the vectorizer on the training data\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['comment_clean'])\n",
        "\n",
        "# Transform the test data using the same fitted vectorizer\n",
        "X_test_vectorized = vectorizer.transform(test_data['comment_clean'])\n",
        "\n",
        "print(f\"Training features shape: {X_train_vectorized.shape}\")\n",
        "print(f\"Testing features shape: {X_test_vectorized.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4659f706-edc0-4a63-98b6-55c16628b3d8",
      "metadata": {
        "id": "4659f706-edc0-4a63-98b6-55c16628b3d8"
      },
      "outputs": [],
      "source": [
        "# Understanding what we just built\n",
        "\n",
        "# Compute word frequencies to find interesting tokens\n",
        "word_frequencies = X_train_vectorized.sum(axis=0).A1\n",
        "top_word_indices = word_frequencies.argsort()[-10:][::-1]\n",
        "top_words = [vectorizer.get_feature_names_out()[i] for i in top_word_indices]\n",
        "\n",
        "# Create a small sample with the top 10 words and first 5 reviews\n",
        "df_bow_sample = pd.DataFrame(\n",
        "    X_train_vectorized[:5, top_word_indices].toarray(),\n",
        "    columns=top_words\n",
        ")\n",
        "\n",
        "print(\"Extract from the Bag-of-Words matrix (first 5 comments, 10 most frequent words):\")\n",
        "df_bow_sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3e76f8-d7ca-44d0-93ef-e63a6d26436c",
      "metadata": {
        "id": "bf3e76f8-d7ca-44d0-93ef-e63a6d26436c"
      },
      "outputs": [],
      "source": [
        "# Train a Naive Bayes classifier\n",
        "\n",
        "naive_bayes_model = MultinomialNB()\n",
        "\n",
        "# Fit the model on the training data\n",
        "naive_bayes_model.fit(X_train_vectorized, train_data['review_score'])\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_nb = naive_bayes_model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Test Set Accuracy: {accuracy_score(test_data['review_score'], y_pred_nb):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(test_data['review_score'], y_pred_nb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423ae45a-44ed-4c68-b8c0-96b055413b37",
      "metadata": {
        "id": "423ae45a-44ed-4c68-b8c0-96b055413b37"
      },
      "outputs": [],
      "source": [
        "# Generate predictions on the test set\n",
        "y_pred = naive_bayes_model.predict(X_test_vectorized)\n",
        "\n",
        "# Display the first few predictions\n",
        "print(f\"First 10 predictions on test set: {y_pred[:10]}\")\n",
        "\n",
        "# Evaluate the model performance\n",
        "test_accuracy = accuracy_score(test_data['review_score'], y_pred)\n",
        "classification_rep = classification_report(test_data['review_score'], y_pred)\n",
        "confusion_mat = confusion_matrix(test_data['review_score'], y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy Score: {test_accuracy:.4f}\\n\")\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_rep)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\\n\")\n",
        "print(confusion_mat)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08fb42b4-ba44-4bd1-bac6-e7292802cee1",
      "metadata": {
        "id": "08fb42b4-ba44-4bd1-bac6-e7292802cee1"
      },
      "source": [
        "## D - Building a More Advanced Model - Logistic Regression with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a111d47-b3a2-4262-9b1b-14fd599d20e0",
      "metadata": {
        "id": "4a111d47-b3a2-4262-9b1b-14fd599d20e0"
      },
      "outputs": [],
      "source": [
        "# Use TF-IDF Vectorizer for better feature extraction\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf.fit_transform(train_data['comment_clean'])\n",
        "\n",
        "# Transform the test data\n",
        "X_test_tfidf = tfidf.transform(test_data['comment_clean'])\n",
        "\n",
        "# Print feature shapes\n",
        "print(f\"Training features shape (TF-IDF): {X_train_tfidf.shape}\")\n",
        "print(f\"Testing features shape (TF-IDF): {X_test_tfidf.shape}\")\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "logistic_model.fit(X_train_tfidf, train_data['review_score'])\n",
        "\n",
        "# Make predictions\n",
        "y_pred_logistic = logistic_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_logistic = accuracy_score(test_data['review_score'], y_pred_logistic)\n",
        "classification_logistic = classification_report(test_data['review_score'], y_pred_logistic, zero_division=0)\n",
        "confusion_logistic = confusion_matrix(test_data['review_score'], y_pred_logistic)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_logistic:.4f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report for Logistic Regression:\\n\")\n",
        "print(classification_logistic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e8456f-cd28-4dfb-a271-4af9bf56cd03",
      "metadata": {
        "id": "d9e8456f-cd28-4dfb-a271-4af9bf56cd03"
      },
      "outputs": [],
      "source": [
        "# Generate and visualize the confusion matrix for Logistic Regression\n",
        "\n",
        "conf_matrix = confusion_matrix(test_data['review_score'], y_pred_logistic)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    conf_matrix, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Blues', \n",
        "    xticklabels=logistic_model.classes_, \n",
        "    yticklabels=logistic_model.classes_\n",
        ")\n",
        "\n",
        "plt.title('Confusion Matrix - Logistic Regression', fontsize=16)\n",
        "plt.xlabel('Predicted Class', fontsize=14)\n",
        "plt.ylabel('Actual Class', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7984681e-831d-4418-b5fd-7a7fbddcfd94",
      "metadata": {
        "id": "7984681e-831d-4418-b5fd-7a7fbddcfd94"
      },
      "outputs": [],
      "source": [
        "# Get feature importance from Logistic Regression model\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': tfidf_vectorizer.get_feature_names_out(),\n",
        "    'importance': lr_classifier.coef_.mean(axis=0)\n",
        "})\n",
        "\n",
        "# Sort by absolute importance\n",
        "feature_importance['abs_importance'] = abs(feature_importance['importance'])\n",
        "feature_importance = feature_importance.sort_values('abs_importance', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb5cde2b-faac-44e6-8dd9-74b47a619af2",
      "metadata": {
        "id": "eb5cde2b-faac-44e6-8dd9-74b47a619af2"
      },
      "outputs": [],
      "source": [
        "# Display the most influential words for positive and negative sentiment\n",
        "\n",
        "print(\"Top 10 words associated with positive sentiment:\")\n",
        "top_positive_words = feature_importance_df.sort_values('importance', ascending=False).head(10)\n",
        "print(top_positive_words[['feature', 'importance']])\n",
        "\n",
        "print(\"\\nTop 10 words associated with negative sentiment:\")\n",
        "top_negative_words = feature_importance_df.sort_values('importance', ascending=True).head(10)\n",
        "print(top_negative_words[['feature', 'importance']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87dc0c0b-8939-4d01-982d-6fe7215b0b24",
      "metadata": {
        "id": "87dc0c0b-8939-4d01-982d-6fe7215b0b24"
      },
      "outputs": [],
      "source": [
        "# Visualize the top features influencing sentiment\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Combine top positive and negative features\n",
        "top_words = pd.concat([top_positive_words.head(15), top_negative_words.head(15)])\n",
        "\n",
        "# Define colors: green for positive, red for negative\n",
        "bar_colors = ['green' if val > 0 else 'red' for val in top_words['importance']]\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.barh(top_words['feature'], top_words['importance'], color=bar_colors)\n",
        "\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "plt.title('Top Positive and Negative Words Impacting Sentiment', fontsize=16)\n",
        "plt.xlabel('Importance', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e049b3-9d28-4001-b034-1d4de518d05d",
      "metadata": {
        "id": "02e049b3-9d28-4001-b034-1d4de518d05d"
      },
      "source": [
        "## E - Rainforest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab26c495-88dd-4195-8171-a13552e1a5f2",
      "metadata": {
        "id": "ab26c495-88dd-4195-8171-a13552e1a5f2"
      },
      "outputs": [],
      "source": [
        "# Start timing to measure model training duration\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize a Random Forest model with limited trees to control training time\n",
        "random_forest_model = RandomForestClassifier(\n",
        "    n_estimators=100,         # number of trees\n",
        "    max_depth=None,           # no limit on tree depth\n",
        "    min_samples_split=2,      # minimum samples required to split a node\n",
        "    random_state=42,\n",
        "    n_jobs=-1                 # utilize all processors\n",
        ")\n",
        "\n",
        "# Train the Random Forest on TF-IDF features\n",
        "random_forest_model.fit(X_train_tfidf, train_data['review_score'])\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_pred_rf_train = random_forest_model.predict(X_train_tfidf)\n",
        "\n",
        "# Compute and display the training time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Training time for Random Forest: {elapsed_time:.2f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b50a9a-db24-46ee-9551-6d85f1464efd",
      "metadata": {
        "id": "82b50a9a-db24-46ee-9551-6d85f1464efd"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy_rf = accuracy_score(test_df['review_score'], y_pred_rf)\n",
        "print(f\"Random Forest Test Accuracy: {accuracy_rf:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(test_df['review_score'], y_pred_rf))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_rf = confusion_matrix(test_df['review_score'], y_pred_rf)\n",
        "\n",
        "# Display confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=rf_classifier.classes_,\n",
        "            yticklabels=rf_classifier.classes_)\n",
        "plt.title('Confusion Matrix - Random Forest', fontsize=16)\n",
        "plt.xlabel('Predicted Label', fontsize=14)\n",
        "plt.ylabel('True Label', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "566a518b-fe1d-42e8-b75f-ff021f2480b5",
      "metadata": {
        "id": "566a518b-fe1d-42e8-b75f-ff021f2480b5"
      },
      "outputs": [],
      "source": [
        "# Display the test accuracy for the Random Forest model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = random_forest_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute the accuracy\n",
        "rf_accuracy = accuracy_score(test_data['review_score'], y_pred_rf)\n",
        "print(f\"Accuracy of Random Forest on Test Set: {rf_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49d91ab1-fa4b-48a4-bda5-a473914ea851",
      "metadata": {
        "id": "49d91ab1-fa4b-48a4-bda5-a473914ea851"
      },
      "outputs": [],
      "source": [
        "# Generate and display the confusion matrix for the Random Forest model\n",
        "\n",
        "conf_matrix_rf = confusion_matrix(test_data['review_score'], y_pred_rf)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    conf_matrix_rf,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=random_forest_model.classes_,\n",
        "    yticklabels=random_forest_model.classes_\n",
        ")\n",
        "\n",
        "plt.title('Confusion Matrix - Random Forest', fontsize=16)\n",
        "plt.xlabel('Predicted Class', fontsize=14)\n",
        "plt.ylabel('Actual Class', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4a2a20-98e3-4cf5-8353-516f2b99c275",
      "metadata": {
        "id": "4c4a2a20-98e3-4cf5-8353-516f2b99c275"
      },
      "outputs": [],
      "source": [
        "# get the feature importance of the Random Forest model\n",
        "\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a DataFrame to hold the feature names and their corresponding importance values\n",
        "import pandas as pd\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': count_vectorizer.get_feature_names_out(),\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the features by their importance in descending order\n",
        "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the top 20 important features\n",
        "print(features_df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ea15c0-8a49-430c-bdf9-b9632c920c13",
      "metadata": {
        "id": "a1ea15c0-8a49-430c-bdf9-b9632c920c13"
      },
      "outputs": [],
      "source": [
        "# Visualize the top important features from the Random Forest model\n",
        "\n",
        "# Select the top 20 features\n",
        "top_rf_features = features_importance_df.head(20)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x='importance', \n",
        "    y='feature', \n",
        "    data=top_rf_features, \n",
        "    palette='viridis', \n",
        "    legend=False\n",
        ")\n",
        "\n",
        "# Add plot title and axis labels\n",
        "plt.title('Top 20 Important Features - Random Forest', fontsize=16)\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.ylabel('Feature Name', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccace26-67e8-4816-b242-cabc114c3d7c",
      "metadata": {
        "id": "7ccace26-67e8-4816-b242-cabc114c3d7c"
      },
      "source": [
        "## F - Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3d0779-9b18-46a5-9a46-83e661045caf",
      "metadata": {
        "id": "fd3d0779-9b18-46a5-9a46-83e661045caf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compare the performances of the 2 models\n",
        "models = ['Naive Bayes', 'Logistic Regression', 'Random Forest']\n",
        "accuracies = [accuracy_nb, accuracy_lr, accuracy_rf]\n",
        "\n",
        "# Create a DataFrame for easy plotting\n",
        "performance_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': accuracies\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Accuracy', y='Model', data=performance_df, palette='Blues_d')\n",
        "\n",
        "plt.title('Comparison of Model Performances', fontsize=16)\n",
        "plt.xlabel('Accuracy', fontsize=12)\n",
        "plt.ylabel('Model', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83989e5d-1963-4ddc-8d1f-94eb176cc9d2",
      "metadata": {
        "id": "83989e5d-1963-4ddc-8d1f-94eb176cc9d2"
      },
      "outputs": [],
      "source": [
        "# Compute the F1 score by class (f1_nb, f1_lr, f1_rf)\n",
        "\n",
        "f1_nb = f1_score(test_df['review_score'], y_pred_nb, average=None)\n",
        "f1_lr = f1_score(test_df['review_score'], y_pred_lr, average=None)\n",
        "f1_rf = f1_score(test_df['review_score'], y_pred_rf, average=None)\n",
        "\n",
        "# Create a dataframe for the visualization\n",
        "f1_df = pd.DataFrame({\n",
        "    'Naive Bayes': f1_nb,\n",
        "    'Logistic Regression': f1_lr,\n",
        "    'Random Forest': f1_rf\n",
        "}, index=rf_classifier.classes_)\n",
        "\n",
        "# Visualize F1 scores by classes\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.heatmap(f1_df, annot=True, cmap='viridis', fmt='.2f', cbar=True)\n",
        "\n",
        "plt.title('F1 Scores by Class for Different Models', fontsize=16)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Class', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143a5b42-a6ea-4502-8322-93ed3833f854",
      "metadata": {
        "id": "143a5b42-a6ea-4502-8322-93ed3833f854"
      },
      "source": [
        "## G - Example use case: predicting new comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13014c2d-8eb5-4022-8028-5f8fed78027e",
      "metadata": {
        "id": "13014c2d-8eb5-4022-8028-5f8fed78027e"
      },
      "outputs": [],
      "source": [
        "# Create a function to predict sentiment for new reviews\n",
        "def predict_sentiment(review_text, vectorizer, model):\n",
        "    # Preprocess the review\n",
        "    processed_review = preprocess_text(review_text)\n",
        "    # Vectorize the review\n",
        "    review_vector = vectorizer.transform([processed_review])\n",
        "    # Predict the sentiment\n",
        "    sentiment = model.predict(review_vector)[0]\n",
        "    # Get prediction probabilities\n",
        "    proba = model.predict_proba(review_vector)[0]\n",
        "    # Return the sentiment and confidence\n",
        "    return sentiment, proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220aa0c3-46af-443b-8405-0f6abceef8ca",
      "metadata": {
        "id": "220aa0c3-46af-443b-8405-0f6abceef8ca"
      },
      "outputs": [],
      "source": [
        "# Example reviews for sentiment prediction\n",
        "\n",
        "# Define a new TF-IDF vectorizer and fit it on training data\n",
        "example_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "example_vectorizer.fit(train_data['comment_clean'])\n",
        "\n",
        "# List of sample reviews to test\n",
        "sample_reviews = [\n",
        "    \"O produto é excelente, superou minhas expectativas!\",  # Positive\n",
        "    \"Entrega foi feita no prazo, mas o produto não é tão bom quanto esperava.\",  # Neutral\n",
        "    \"Péssimo produto, chegou com defeito e o atendimento ao cliente foi horrível.\"  # Negative\n",
        "]\n",
        "\n",
        "# Predict sentiment for each example\n",
        "print(\"Sentiment prediction for sample reviews:\")\n",
        "for idx, review in enumerate(sample_reviews, start=1):\n",
        "    pred_sentiment, pred_proba = predict_sentiment(review, example_vectorizer, logistic_model)\n",
        "    print(f\"\\nReview {idx}: {review}\")\n",
        "    print(f\"Predicted sentiment: {pred_sentiment}\")\n",
        "    print(f\"Confidence: {max(pred_proba):.2f}\")\n",
        "    print(f\"Class probabilities: {dict(zip(logistic_model.classes_, pred_proba))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c150d3-4bdd-4646-8514-091b3e600596",
      "metadata": {
        "id": "77c150d3-4bdd-4646-8514-091b3e600596"
      },
      "source": [
        "# II - Delivery prediction (bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0838558-ba0b-4b7a-ba5b-c4bad1b6ccbb",
      "metadata": {
        "id": "d0838558-ba0b-4b7a-ba5b-c4bad1b6ccbb"
      },
      "source": [
        "## A - Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6496fb11-5c89-43eb-9c87-0bc24abb573d",
      "metadata": {
        "id": "6496fb11-5c89-43eb-9c87-0bc24abb573d"
      },
      "outputs": [],
      "source": [
        "# We'll use the merged dataframe from previous parts\n",
        "# If not already done, we need to merge the necessary dataframes\n",
        "if 'df' not in globals():\n",
        "    # Load required datasets\n",
        "    df_orders = pd.read_csv(raw_path + 'olist_orders_dataset.csv')\n",
        "    df_customers = pd.read_csv(raw_path + 'olist_customers_dataset.csv')\n",
        "    df_order_items = pd.read_csv(raw_path + 'olist_order_items_dataset.csv')\n",
        "    df_products = pd.read_csv(raw_path + 'olist_products_dataset.csv')\n",
        "\n",
        "    # Merge datasets\n",
        "    df = df_orders.merge(df_customers, on='customer_id')\n",
        "    df = df.merge(df_order_items, on='order_id')\n",
        "    df = df.merge(df_products, on='product_id')\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c40206d9-080c-4d5c-a05c-2a6b2725e9ce",
      "metadata": {
        "id": "c40206d9-080c-4d5c-a05c-2a6b2725e9ce"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to datetime\n",
        "date_columns = ['order_purchase_timestamp', 'order_approved_at',\n",
        "                'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
        "                'order_estimated_delivery_date']\n",
        "\n",
        "for col in date_columns:\n",
        "    df[col] = pd.to_datetime(df[col])\n",
        "\n",
        "\n",
        "# Calculate delivery time in days\n",
        "df['actual_delivery_time'] = (df['order_delivered_customer_date'] -\n",
        "                             df['order_purchase_timestamp']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "# Calculate if the delivery was delayed (1) or not (0)\n",
        "df['estimated_delivery_time'] = (df['order_estimated_delivery_date'] -\n",
        "                                df['order_purchase_timestamp']).dt.total_seconds() / (24 * 3600)\n",
        "df['is_delayed'] = (df['actual_delivery_time'] > df['estimated_delivery_time']).astype(int)\n",
        "\n",
        "# Calculate time to carrier in days\n",
        "df['time_to_carrier'] = (df['order_delivered_carrier_date'] -\n",
        "                        df['order_purchase_timestamp']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "# Filter out rows with missing delivery dates (canceled orders, etc.)\n",
        "delivery_df = df.dropna(subset=['order_delivered_customer_date', 'order_delivered_carrier_date'])\n",
        "\n",
        "delivery_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacd295c-6239-402b-8de4-803631ee3e7c",
      "metadata": {
        "id": "dacd295c-6239-402b-8de4-803631ee3e7c"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='is_delayed', data=delivery_df, palette='viridis')\n",
        "plt.title('Distribution of Delayed vs On-time Deliveries', fontsize=16)\n",
        "plt.xlabel('Is Delayed (1 = Yes, 0 = No)', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.xticks([0, 1], ['On-time', 'Delayed'])\n",
        "for i, count in enumerate(delivery_df['is_delayed'].value_counts()):\n",
        "    plt.text(i, count + 100, f\"{count} ({count/len(delivery_df):.1%})\",\n",
        "             ha='center', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b2ca9f-f690-46ae-82ba-847c01c68f61",
      "metadata": {
        "id": "c2b2ca9f-f690-46ae-82ba-847c01c68f61"
      },
      "source": [
        "## B - Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f44fd836-62de-4029-be17-180c9b140ce1",
      "metadata": {
        "id": "f44fd836-62de-4029-be17-180c9b140ce1"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_df = delivery_df.copy()\n",
        "\n",
        "# Extract temporal features\n",
        "model_df['purchase_hour'] = model_df['order_purchase_timestamp'].dt.hour\n",
        "model_df['purchase_day'] = model_df['order_purchase_timestamp'].dt.day\n",
        "model_df['purchase_month'] = model_df['order_purchase_timestamp'].dt.month\n",
        "model_df['purchase_year'] = model_df['order_purchase_timestamp'].dt.year\n",
        "model_df['purchase_dayofweek'] = model_df['order_purchase_timestamp'].dt.dayofweek\n",
        "model_df['purchase_weekend'] = (model_df['purchase_dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# Calculate distance between customer and seller (using zip code prefix as a proxy)\n",
        "model_df['zip_distance'] = abs(model_df['customer_zip_code_prefix'] - model_df['seller_zip_code_prefix'])\n",
        "\n",
        "# Calculate price per weight\n",
        "model_df['price_per_weight'] = model_df['price'] / model_df['product_weight_g'].replace(0, 0.1)\n",
        "\n",
        "# Create product volume feature\n",
        "model_df['product_volume'] = (model_df['product_length_cm'] *\n",
        "                             model_df['product_height_cm'] *\n",
        "                             model_df['product_width_cm'])\n",
        "\n",
        "# Handle infinite values\n",
        "model_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Display the first few rows of the engineered features\n",
        "print(\"Sample of engineered features:\")\n",
        "model_df[['purchase_hour', 'purchase_day', 'purchase_month', 'purchase_year',\n",
        "                'purchase_dayofweek', 'purchase_weekend', 'zip_distance',\n",
        "                'price_per_weight', 'product_volume']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba36e4d-b01e-434a-8c7d-32e60569da7b",
      "metadata": {
        "id": "cba36e4d-b01e-434a-8c7d-32e60569da7b"
      },
      "source": [
        "## C - Exploratory Data Analysis for Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4504a8-e1c8-462f-bb72-9851c9989975",
      "metadata": {
        "id": "6e4504a8-e1c8-462f-bb72-9851c9989975"
      },
      "outputs": [],
      "source": [
        "# Analyze correlation between features and delivery time with a correlation matrix\n",
        "\n",
        "'your code here'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ec5a22-f4c1-4d8d-a2e0-3b1d11babd95",
      "metadata": {
        "id": "95ec5a22-f4c1-4d8d-a2e0-3b1d11babd95"
      },
      "outputs": [],
      "source": [
        "# Analyze categorical features\n",
        "categorical_features = ['customer_state', 'seller_state', 'product_category_name']\n",
        "\n",
        "# compute the average delivery time for each of the categorical feature with a bar chart\n",
        "\n",
        "'your code here'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011d9019-ec8c-4e6e-841e-313ee386582e",
      "metadata": {
        "id": "011d9019-ec8c-4e6e-841e-313ee386582e"
      },
      "source": [
        "## D - Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef8e1f2-2e79-43c3-85a4-99f6c11faa9b",
      "metadata": {
        "id": "3ef8e1f2-2e79-43c3-85a4-99f6c11faa9b"
      },
      "outputs": [],
      "source": [
        "# Select features based on correlation analysis and domain knowledge\n",
        "selected_numeric_features = ['freight_value', 'price', 'product_weight_g',\n",
        "                            'product_volume', 'zip_distance', 'time_to_carrier',\n",
        "                            'purchase_month', 'purchase_dayofweek']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0d5078-73f8-4961-b50a-5ae2bc99d10a",
      "metadata": {
        "id": "ef0d5078-73f8-4961-b50a-5ae2bc99d10a"
      },
      "outputs": [],
      "source": [
        "# Prepare feature and target variables\n",
        "X_numeric = model_df[selected_numeric_features]\n",
        "X_categorical = model_df[selected_categorical_features]\n",
        "X_combined = pd.concat([X_numeric, X_categorical], axis=1)\n",
        "\n",
        "# For regression task (predicting delivery time)\n",
        "y_regression = model_df['actual_delivery_time']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_reg_train, y_reg_test = train_test_split(\n",
        "    X_combined, y_regression, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3a4194-ec96-43c4-9439-6364ddff7c3a",
      "metadata": {
        "id": "ca3a4194-ec96-43c4-9439-6364ddff7c3a"
      },
      "source": [
        "## E - Building the Regression Model (Predicting Delivery Time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89aa0dc-05f2-4dca-ad44-2ea79c7685bb",
      "metadata": {
        "id": "e89aa0dc-05f2-4dca-ad44-2ea79c7685bb"
      },
      "outputs": [],
      "source": [
        "# Define preprocessing for numeric features\n",
        "\n",
        "'your code here'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2484af5d-b0fb-4d40-8fe1-5785c7fbe673",
      "metadata": {
        "id": "2484af5d-b0fb-4d40-8fe1-5785c7fbe673"
      },
      "outputs": [],
      "source": [
        "# Create a Linear Regression Pipeline\n",
        "\n",
        "'your code here'\n",
        "\n",
        "# Train and evaluate Linear Regression\n",
        "\n",
        "'your code here'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903f3fdd-3e14-4f08-848a-b51985e0dbab",
      "metadata": {
        "id": "903f3fdd-3e14-4f08-848a-b51985e0dbab"
      },
      "outputs": [],
      "source": [
        "# Make predictions based on your linear regression\n",
        "\n",
        "'your code here'\n",
        "\n",
        "# Evaluate the model\n",
        "mae_lr = 'your code here'\n",
        "rmse_lr = 'your code here'\n",
        "r2_lr = 'your code here'\n",
        "\n",
        "print(f\"Linear Regression - MAE: {mae_lr:.2f} days\")\n",
        "print(f\"Linear Regression - RMSE: {rmse_lr:.2f} days\")\n",
        "print(f\"Linear Regression - R²: {r2_lr:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d08f062-047d-4d37-8d90-bd843b86a2fe",
      "metadata": {
        "id": "0d08f062-047d-4d37-8d90-bd843b86a2fe"
      },
      "source": [
        "the R² should be about 0.40, which is not a good number. Can you do better with a Random Forest model, or any other model?\n",
        "\n",
        "Objective is to get a R² > 0.45!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7769b6d5-8e84-472d-a9d0-72321867ff4f",
      "metadata": {
        "id": "7769b6d5-8e84-472d-a9d0-72321867ff4f"
      },
      "source": [
        "## F - Building a Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dcef83a-528b-4692-a0c7-aa5bd5df9ee7",
      "metadata": {
        "id": "6dcef83a-528b-4692-a0c7-aa5bd5df9ee7"
      },
      "outputs": [],
      "source": [
        "'your code here'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af09ba80-7345-4233-817a-6cb7f0247840",
      "metadata": {
        "id": "af09ba80-7345-4233-817a-6cb7f0247840"
      },
      "source": [
        "### G - Building a K-Nearest Neighbors model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e58aa9-c84f-4feb-9a61-2480340cd129",
      "metadata": {
        "id": "66e58aa9-c84f-4feb-9a61-2480340cd129"
      },
      "outputs": [],
      "source": [
        "'your code here'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
